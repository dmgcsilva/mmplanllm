import os
import numpy as np
import pandas as pd
from PIL import Image, ImageFont, ImageDraw
from torchvision.transforms import functional as F
from torch.utils.data import Dataset
from torch.utils.data import Sampler
from transformers import AutoFeatureExtractor
import transformers
import torch


class CC3MDataset(Dataset):

    def __init__(self, tokenizer: transformers.PreTrainedTokenizer, data_path: str, debug=False, **kwargs):

        self.kwargs = kwargs
        self.debug = debug

        df = pd.read_csv(data_path, sep="\t")

        self.base_image_dir = kwargs.get('base_image_dir', '')
        self.images = df['local_path'].tolist()
        self.captions = df['caption'].tolist()
        assert len(self.images) == len(self.captions)

        self.feature_extractor_model = kwargs.get('feature_extractor_model', 'google/vit-base-patch16-224')
        self.image_size = kwargs.get('image_size', 224)
        self.feature_extractor = AutoFeatureExtractor.from_pretrained(self.feature_extractor_model,
                                                                      do_resize=True,
                                                                      size=self.image_size)
        self.tokenizer = tokenizer
        self.retrieval_token_idx = tokenizer.get_vocab().get('[RET]')
        self.image_token_idx = tokenizer.cls_token_id

        print(f"Retrieval token idx: {self.retrieval_token_idx}")
        print(f"Image token idx: {self.image_token_idx}")
        print(f"Tokenizer padding side: {self.tokenizer.padding_side}")
        print(f"Tokenizer truncation side: {self.tokenizer.truncation_side}")

        assert self.retrieval_token_idx is not None, "Tokenizer must have a [RET] token"
        assert self.image_token_idx is not None, "Tokenizer must have a [CLS] token"

        self.max_len = kwargs.get('max_len', self.tokenizer.model_max_length)
        print(f"Max len: {self.max_len}")

        self.font = None

        self.is_test = kwargs.get('is_test', False) or "_val" in data_path or "_test" in data_path or "_eval" in data_path


    def __len__(self):
        return len(self.captions)

    def __getitem__(self, idx):
        while True:

            image_path = os.path.join(self.base_image_dir, str(self.images[idx]))
            caption = str(self.captions[idx])
            try:
                img = Image.open(image_path)
                images = self.feature_extractor(img.convert('RGB'), return_tensors="pt").pixel_values[0, ...]

                caption = f"{self.tokenizer.cls_token}{caption}" if self.is_test else f"{self.tokenizer.cls_token}{caption}[RET]"

                tokenized_data = self.tokenizer(
                    caption,
                    return_tensors="pt",
                    padding='max_length' if not self.is_test else 'do_not_pad',
                    truncation=True,
                    max_length=self.max_len,
                )
                tokens = tokenized_data.input_ids[0]
                attn_mask = tokenized_data.attention_mask[0]


                if self.is_test:
                    # for test mode, we do not need to add the RET or EOS tokens as they should be generated by the model
                    caption_len = tokenized_data.attention_mask[0].sum()
                    assert all([t != self.retrieval_token_idx for t in tokens])
                else:
                    caption_len = tokenized_data.attention_mask[0].sum() # subtract 1 to account for the eos token

                    if tokens[caption_len-1] not in [self.retrieval_token_idx]:
                        tokens[caption_len-1] = self.retrieval_token_idx

                    # assert that the token at caption_len is the [RET] token and that the first token in the [IMG] token
                    assert tokens[caption_len-1] == self.retrieval_token_idx, f"Token at caption_len is not [RET] token: {tokens[caption_len-1]}"

                assert tokens[1] == self.image_token_idx or tokens[0] == self.image_token_idx, f"Token at 0 or 1 is not [IMG] token: {tokens} caption: {caption}"

                return dict(images=images, tgt_tokens=tokens, caption_len=caption_len,
                            supported_tasks=['retrieval', 'captioning'])

            except Exception as e:
                # raise e
                print(f'Error reading {image_path} with caption {caption}: {e}')
                # Pick a new example at random.
                idx = np.random.randint(0, len(self) - 1)

    def get_all_visual_embs(self, model):
        pixel_values = []
        for img in self.images:
            try:
                images = self.feature_extractor(Image.open(os.path.join(self.base_image_dir, str(img))).convert('RGB'), return_tensors="pt").pixel_values[0, ...]
                pixel_values.append(images.unsqueeze(0))
            except Exception as e:
                print(f'Error reading {img}: {e}, continuing...')
                continue

        for i in range(len(pixel_values)):
            pixel_values[i] = model.model.get_visual_embs(pixel_values[i], mode='retrieval')

        return torch.stack(pixel_values, dim=0).squeeze(2)


def create_image_of_text(text: str, width: int = 224, nrows: int = 2, color=(255, 255, 255), font=None) -> torch.Tensor:
    """Creates a (3, nrows * 14, width) image of text.

    Returns:
      cap_img: (3, 14 * nrows, width) image of wrapped text.
    """
    height = 12
    padding = 5
    effective_width = width - 2 * padding
    # Create a black image to draw text on.
    cap_img = Image.new('RGB', (effective_width * nrows, height), color=(0, 0, 0))
    draw = ImageDraw.Draw(cap_img)
    draw.text((0, 0), text, color, font=font or ImageFont.load_default())
    cap_img = F.convert_image_dtype(F.pil_to_tensor(cap_img), torch.float32)  # (3, height, W * nrows)
    cap_img = torch.split(cap_img, effective_width, dim=-1)  # List of nrow elements of shape (3, height, W)
    cap_img = torch.cat(cap_img, dim=1)  # (3, height * nrows, W)
    # Add zero padding.
    cap_img = torch.nn.functional.pad(cap_img, [padding, padding, 0, padding])
    return cap_img

